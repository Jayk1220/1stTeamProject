{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f93dcd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad712a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#자동차 산업\n",
    "    #창 열기\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.yna.co.kr/industry/automobile')\n",
    "driver.implicitly_wait(0.5)\n",
    "    \n",
    "    #기사 제목과 link 수집\n",
    "car_articles = []\n",
    "\n",
    "for page in range(1, 21):\n",
    "#     print(f\"페이지 수집 중: {page}\")\n",
    "\n",
    "    # 현재 URL 기준으로 page 파라미터만 변경\n",
    "    url = f\"{driver.current_url.split('?')[0]}?page={page}\"\n",
    "    driver.get(url)\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "    lis = driver.find_elements(By.CSS_SELECTOR, \"li[data-cid]\")\n",
    "\n",
    "    for li in lis:\n",
    "        try:\n",
    "            a = li.find_element(By.CSS_SELECTOR, \"strong.tit-wrap a.tit-news\")\n",
    "            title = a.find_element(By.CSS_SELECTOR, \"span.title01\").text.strip()\n",
    "            link = a.get_attribute(\"href\")\n",
    "            date = li.find_element(By.CSS_SELECTOR, \"span.txt-time\").text.strip()\n",
    "\n",
    "            car_articles.append({\n",
    "                \"date\": date,\n",
    "                \"title\": title,\n",
    "                \"url\": link\n",
    "            })\n",
    "\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    return \" \".join((s or \"\").split())\n",
    "\n",
    "def extract_yna_body(driver):\n",
    "    root = driver.find_element(By.CSS_SELECTOR, \"div.story-news.article\")\n",
    "\n",
    "    # 본문은 p들로 구성되어 있으니 p만 모아서 합침\n",
    "    ps = root.find_elements(By.CSS_SELECTOR, \"p\")\n",
    "\n",
    "    chunks = []\n",
    "    for p in ps:\n",
    "        txt = clean_text(p.text)\n",
    "        if not txt:\n",
    "            continue\n",
    "\n",
    "        # 저작권/제보/AI학습금지 같은 문구 제거\n",
    "        if \"무단 전재\" in txt or \"AI 학습\" in txt or txt.startswith(\"제보는\"):\n",
    "            continue\n",
    "\n",
    "        chunks.append(txt)\n",
    "\n",
    "    return \"\\n\".join(chunks).strip()\n",
    "\n",
    "# 상세 본문 수집\n",
    "for i, item in enumerate(car_articles, start=1):\n",
    "    driver.get(item[\"url\"])\n",
    "    time.sleep(0.8)\n",
    "\n",
    "    try:\n",
    "        item[\"article\"] = extract_yna_body(driver)\n",
    "#         print(f\"[본문] {i}/{len(articles)} OK\")\n",
    "\n",
    "    except Exception as e:\n",
    "        item[\"article\"] = \"\"\n",
    "        print(f\"[본문] {i}/{len(car_articles)} FAIL - {e}\")\n",
    "\n",
    "print(\"수집 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca65095",
   "metadata": {},
   "outputs": [],
   "source": [
    "#건설 산업\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.yna.co.kr/industry/construction')\n",
    "driver.implicitly_wait(0.5)\n",
    "\n",
    "    #기사 제목과 link 수집\n",
    "construct_articles = []\n",
    "\n",
    "for page in range(1, 21):\n",
    "#     print(f\"페이지 수집 중: {page}\")\n",
    "\n",
    "    # 현재 URL 기준으로 page 파라미터만 변경\n",
    "    url = f\"{driver.current_url.split('?')[0]}?page={page}\"\n",
    "    driver.get(url)\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "    lis = driver.find_elements(By.CSS_SELECTOR, \"li[data-cid]\")\n",
    "\n",
    "    for li in lis:\n",
    "        try:\n",
    "            a = li.find_element(By.CSS_SELECTOR, \"strong.tit-wrap a.tit-news\")\n",
    "            title = a.find_element(By.CSS_SELECTOR, \"span.title01\").text.strip()\n",
    "            link = a.get_attribute(\"href\")\n",
    "            date = li.find_element(By.CSS_SELECTOR, \"span.txt-time\").text.strip()\n",
    "\n",
    "            construct_articles.append({\n",
    "                \"date\": date,\n",
    "                \"title\": title,\n",
    "                \"url\": link\n",
    "            })\n",
    "\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    return \" \".join((s or \"\").split())\n",
    "\n",
    "def extract_yna_body(driver):\n",
    "    root = driver.find_element(By.CSS_SELECTOR, \"div.story-news.article\")\n",
    "\n",
    "    # 본문은 p들로 구성되어 있으니 p만 모아서 합침\n",
    "    ps = root.find_elements(By.CSS_SELECTOR, \"p\")\n",
    "\n",
    "    chunks = []\n",
    "    for p in ps:\n",
    "        txt = clean_text(p.text)\n",
    "        if not txt:\n",
    "            continue\n",
    "\n",
    "        # 저작권/제보/AI학습금지 같은 문구 제거\n",
    "        if \"무단 전재\" in txt or \"AI 학습\" in txt or txt.startswith(\"제보는\"):\n",
    "            continue\n",
    "\n",
    "        chunks.append(txt)\n",
    "\n",
    "    return \"\\n\".join(chunks).strip()\n",
    "\n",
    "# 상세 본문 수집\n",
    "for i, item in enumerate(construct_articles, start=1):\n",
    "    driver.get(item[\"url\"])\n",
    "    time.sleep(0.8)\n",
    "\n",
    "    try:\n",
    "        item[\"article\"] = extract_yna_body(driver)\n",
    "#         print(f\"[본문] {i}/{len(articles)} OK\")\n",
    "    except Exception as e:\n",
    "        item[\"article\"] = \"\"\n",
    "        print(f\"[본문] {i}/{len(construct_articles)} FAIL - {e}\")\n",
    "\n",
    "print(\"수집 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c9a3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#헬스케어 산업\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.yna.co.kr/industry/bioindustry-health')\n",
    "driver.implicitly_wait(0.5)\n",
    "    \n",
    "    #기사 제목과 link 수집\n",
    "health_articles = []\n",
    "\n",
    "for page in range(1, 21):\n",
    "#     print(f\"페이지 수집 중: {page}\")\n",
    "\n",
    "    # 현재 URL 기준으로 page 파라미터만 변경\n",
    "    url = f\"{driver.current_url.split('?')[0]}?page={page}\"\n",
    "    driver.get(url)\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "    lis = driver.find_elements(By.CSS_SELECTOR, \"li[data-cid]\")\n",
    "\n",
    "    for li in lis:\n",
    "        try:\n",
    "            a = li.find_element(By.CSS_SELECTOR, \"strong.tit-wrap a.tit-news\")\n",
    "            title = a.find_element(By.CSS_SELECTOR, \"span.title01\").text.strip()\n",
    "            link = a.get_attribute(\"href\")\n",
    "            date = li.find_element(By.CSS_SELECTOR, \"span.txt-time\").text.strip()\n",
    "\n",
    "            health_articles.append({\n",
    "                \"date\": date,\n",
    "                \"title\": title,\n",
    "                \"url\": link\n",
    "            })\n",
    "\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    return \" \".join((s or \"\").split())\n",
    "\n",
    "def extract_yna_body(driver):\n",
    "    root = driver.find_element(By.CSS_SELECTOR, \"div.story-news.article\")\n",
    "\n",
    "    # 본문은 p들로 구성되어 있으니 p만 모아서 합침\n",
    "    ps = root.find_elements(By.CSS_SELECTOR, \"p\")\n",
    "\n",
    "    chunks = []\n",
    "    for p in ps:\n",
    "        txt = clean_text(p.text)\n",
    "        if not txt:\n",
    "            continue\n",
    "\n",
    "        # 저작권/제보/AI학습금지 같은 문구 제거\n",
    "        if \"무단 전재\" in txt or \"AI 학습\" in txt or txt.startswith(\"제보는\"):\n",
    "            continue\n",
    "\n",
    "        chunks.append(txt)\n",
    "\n",
    "    return \"\\n\".join(chunks).strip()\n",
    "\n",
    "# 상세 본문 수집\n",
    "for i, item in enumerate(health_articles, start=1):\n",
    "    driver.get(item[\"url\"])\n",
    "    time.sleep(0.8)\n",
    "\n",
    "    try:\n",
    "        item[\"article\"] = extract_yna_body(driver)\n",
    "#         print(f\"[본문] {i}/{len(articles)} OK\")\n",
    "    except Exception as e:\n",
    "        item[\"article\"] = \"\"\n",
    "        print(f\"[본문] {i}/{len(health_articles)} FAIL - {e}\")\n",
    "\n",
    "print(\"수집 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6c319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#해당없음 산업 (경제)\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.yna.co.kr/economy/all')\n",
    "driver.implicitly_wait(0.5)\n",
    "    \n",
    "    #기사 제목과 link 수집\n",
    "econ_articles = []\n",
    "\n",
    "for page in range(1, 21):\n",
    "#     print(f\"페이지 수집 중: {page}\")\n",
    "\n",
    "    # 현재 URL 기준으로 page 파라미터만 변경\n",
    "    url = f\"{driver.current_url.split('?')[0]}?page={page}\"\n",
    "    driver.get(url)\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "    lis = driver.find_elements(By.CSS_SELECTOR, \"li[data-cid]\")\n",
    "\n",
    "    for li in lis:\n",
    "        try:\n",
    "            a = li.find_element(By.CSS_SELECTOR, \"strong.tit-wrap a.tit-news\")\n",
    "            title = a.find_element(By.CSS_SELECTOR, \"span.title01\").text.strip()\n",
    "            link = a.get_attribute(\"href\")\n",
    "            date = li.find_element(By.CSS_SELECTOR, \"span.txt-time\").text.strip()\n",
    "\n",
    "            econ_articles.append({\n",
    "                \"date\": date,\n",
    "                \"title\": title,\n",
    "                \"url\": link\n",
    "            })\n",
    "\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    return \" \".join((s or \"\").split())\n",
    "\n",
    "def extract_yna_body(driver):\n",
    "    root = driver.find_element(By.CSS_SELECTOR, \"div.story-news.article\")\n",
    "\n",
    "    # 본문은 p들로 구성되어 있으니 p만 모아서 합침\n",
    "    ps = root.find_elements(By.CSS_SELECTOR, \"p\")\n",
    "\n",
    "    chunks = []\n",
    "    for p in ps:\n",
    "        txt = clean_text(p.text)\n",
    "        if not txt:\n",
    "            continue\n",
    "\n",
    "        # 저작권/제보/AI학습금지 같은 문구 제거\n",
    "        if \"무단 전재\" in txt or \"AI 학습\" in txt or txt.startswith(\"제보는\"):\n",
    "            continue\n",
    "\n",
    "        chunks.append(txt)\n",
    "\n",
    "    return \"\\n\".join(chunks).strip()\n",
    "\n",
    "# 상세 본문 수집\n",
    "for i, item in enumerate(econ_articles, start=1):\n",
    "    driver.get(item[\"url\"])\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    try:\n",
    "        item[\"article\"] = extract_yna_body(driver)\n",
    "#         print(f\"[본문] {i}/{len(articles)} OK\")\n",
    "    except Exception as e:\n",
    "        item[\"article\"] = \"\"\n",
    "        print(f\"[본문] {i}/{len(econ_articles)} FAIL - {e}\")\n",
    "\n",
    "print(\"수집 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1292ca7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#해당없음 산업 (사회)\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.yna.co.kr/society/all')\n",
    "driver.implicitly_wait(0.5)\n",
    "\n",
    "social_articles = []\n",
    "    #기사 제목과 link 수집\n",
    "\n",
    "for page in range(1, 21):\n",
    "#     print(f\"페이지 수집 중: {page}\")\n",
    "\n",
    "    # 현재 URL 기준으로 page 파라미터만 변경\n",
    "    url = f\"{driver.current_url.split('?')[0]}?page={page}\"\n",
    "    driver.get(url)\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "    lis = driver.find_elements(By.CSS_SELECTOR, \"li[data-cid]\")\n",
    "\n",
    "    for li in lis:\n",
    "        try:\n",
    "            a = li.find_element(By.CSS_SELECTOR, \"strong.tit-wrap a.tit-news\")\n",
    "            title = a.find_element(By.CSS_SELECTOR, \"span.title01\").text.strip()\n",
    "            link = a.get_attribute(\"href\")\n",
    "            date = li.find_element(By.CSS_SELECTOR, \"span.txt-time\").text.strip()\n",
    "\n",
    "            social_articles.append({\n",
    "                \"date\": date,\n",
    "                \"title\": title,\n",
    "                \"url\": link\n",
    "            })\n",
    "\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    return \" \".join((s or \"\").split())\n",
    "\n",
    "def extract_yna_body(driver):\n",
    "    root = driver.find_element(By.CSS_SELECTOR, \"div.story-news.article\")\n",
    "\n",
    "    # 본문은 p들로 구성되어 있으니 p만 모아서 합침\n",
    "    ps = root.find_elements(By.CSS_SELECTOR, \"p\")\n",
    "\n",
    "    chunks = []\n",
    "    for p in ps:\n",
    "        txt = clean_text(p.text)\n",
    "        if not txt:\n",
    "            continue\n",
    "\n",
    "        # 저작권/제보/AI학습금지 같은 문구 제거\n",
    "        if \"무단 전재\" in txt or \"AI 학습\" in txt or txt.startswith(\"제보는\"):\n",
    "            continue\n",
    "\n",
    "        chunks.append(txt)\n",
    "\n",
    "    return \"\\n\".join(chunks).strip()\n",
    "\n",
    "# 상세 본문 수집\n",
    "for i, item in enumerate(social_articles, start=1):\n",
    "    driver.get(item[\"url\"])\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    try:\n",
    "        item[\"article\"] = extract_yna_body(driver)\n",
    "#         print(f\"[본문] {i}/{len(articles)} OK\")\n",
    "    except Exception as e:\n",
    "        item[\"article\"] = \"\"\n",
    "        print(f\"[본문] {i}/{len(social_articles)} FAIL - {e}\")\n",
    "\n",
    "print(\"수집 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0787db4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#해당없음 산업 (연예)\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.yna.co.kr/entertainment/all')\n",
    "driver.implicitly_wait(0.5)\n",
    "\n",
    "entertain_articles = []\n",
    "    #기사 제목과 link 수집\n",
    "\n",
    "for page in range(1, 21):\n",
    "#     print(f\"페이지 수집 중: {page}\")\n",
    "\n",
    "    # 현재 URL 기준으로 page 파라미터만 변경\n",
    "    url = f\"{driver.current_url.split('?')[0]}?page={page}\"\n",
    "    driver.get(url)\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "    lis = driver.find_elements(By.CSS_SELECTOR, \"li[data-cid]\")\n",
    "\n",
    "    for li in lis:\n",
    "        try:\n",
    "            a = li.find_element(By.CSS_SELECTOR, \"strong.tit-wrap a.tit-news\")\n",
    "            title = a.find_element(By.CSS_SELECTOR, \"span.title01\").text.strip()\n",
    "            link = a.get_attribute(\"href\")\n",
    "            date = li.find_element(By.CSS_SELECTOR, \"span.txt-time\").text.strip()\n",
    "\n",
    "            entertain_articles.append({\n",
    "                \"date\": date,\n",
    "                \"title\": title,\n",
    "                \"url\": link\n",
    "            })\n",
    "\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    return \" \".join((s or \"\").split())\n",
    "\n",
    "def extract_yna_body(driver):\n",
    "    root = driver.find_element(By.CSS_SELECTOR, \"div.story-news.article\")\n",
    "\n",
    "    # 본문은 p들로 구성되어 있으니 p만 모아서 합침\n",
    "    ps = root.find_elements(By.CSS_SELECTOR, \"p\")\n",
    "\n",
    "    chunks = []\n",
    "    for p in ps:\n",
    "        txt = clean_text(p.text)\n",
    "        if not txt:\n",
    "            continue\n",
    "\n",
    "        # 저작권/제보/AI학습금지 같은 문구 제거\n",
    "        if \"무단 전재\" in txt or \"AI 학습\" in txt or txt.startswith(\"제보는\"):\n",
    "            continue\n",
    "\n",
    "        chunks.append(txt)\n",
    "\n",
    "    return \"\\n\".join(chunks).strip()\n",
    "\n",
    "# 상세 본문 수집\n",
    "for i, item in enumerate(entertain_articles, start=1):\n",
    "    driver.get(item[\"url\"])\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    try:\n",
    "        item[\"article\"] = extract_yna_body(driver)\n",
    "#         print(f\"[본문] {i}/{len(articles)} OK\")\n",
    "    except Exception as e:\n",
    "        item[\"article\"] = \"\"\n",
    "        print(f\"[본문] {i}/{len(entertain_articles)} FAIL - {e}\")\n",
    "\n",
    "print(\"수집 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b903b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리스트 데이터프레임화 + 산업 컬럼 추가\n",
    "df_car = pd.DataFrame(car_articles)\n",
    "df_car[\"산업\"]=\"자동차\"\n",
    "df_const = pd.DataFrame(construct_articles)\n",
    "df_const[\"산업\"]=\"건설\"\n",
    "df_health = pd.DataFrame(health_articles)\n",
    "df_health[\"산업\"]=\"헬스케어\"\n",
    "df_econ = pd.DataFrame(econ_articles)\n",
    "df_econ[\"산업\"]=\"해당없음\"\n",
    "df_social = pd.DataFrame(social_articles)\n",
    "df_social[\"산업\"]=\"해당없음\"\n",
    "df_entertain = pd.DataFrame(entertain_articles)\n",
    "df_entertain[\"산업\"]=\"해당없음\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8812621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터프레임 통합\n",
    "yna_df = pd.concat ([df_car, df_const, df_health, df_econ, df_social, df_entertain], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8cb36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#본문 앞 불필요 내용 삭제\n",
    "yna_df['article'] = yna_df['article'].str.replace(\n",
    "    r'.*?기자\\s*=\\s*',\n",
    "    '',\n",
    "    regex=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d404fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 본문 앞 불필요 문자 제거\n",
    "yna_df['article'] = yna_df['article'].str.replace(\n",
    "    r'^.*?\\n',\n",
    "    '',\n",
    "    regex=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f00b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#대괄호와 그 안 정보 제거\n",
    "yna_df['article'] = yna_df['article'].str.replace(\n",
    "    r'\\[.*?\\]',\n",
    "    '',\n",
    "    regex=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ca3b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컬럼 순서 정돈\n",
    "yna_df = yna_df[\n",
    "    ['date', 'title', 'article','url', '산업']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188712b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 날짜 타입 변환\n",
    "YEAR = 2025\n",
    "\n",
    "yna_df['date'] = pd.to_datetime(\n",
    "    f'{YEAR}-' + yna_df['date'].astype(str).str.split(' ').str[0],\n",
    "    format='%Y-%m-%d'\n",
    ")\n",
    "yna_df['date'] = pd.to_datetime(yna_df['date'])\n",
    "\n",
    "yna_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeaf411",
   "metadata": {},
   "outputs": [],
   "source": [
    "#article이 없는 행은 title로 채우기\n",
    "yna_df['article'] = yna_df['article'].fillna(yna_df['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beaef4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 저장\n",
    "yna_df.to_csv(\n",
    "    \"yna_news.csv\",\n",
    "    index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecc2246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701e36e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205e6a62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42be83c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3756c520",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
